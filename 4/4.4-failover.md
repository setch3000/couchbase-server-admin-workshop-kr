# 4.4 failover 수행

## 1. failover 수행

웹 UI로 전환한 뒤 Server Nodes 섹션에서 세 번째 노드가 Down 상태로 표시되는 것을 확인할 수 있습니다. 세 번째 Down 노드에 대해 `Fail Over`를 클릭하세요.

<figure><img src="../.gitbook/assets/image.png" alt=""><figcaption></figcaption></figure>

Confirm Failover 팝업에서 Confirm failover 옆의 체크박스를 선택한 뒤, Failover Node 버튼을 눌러 노드를 페일오버(Failover)하십시오.

<figure><img src="../.gitbook/assets/image (1).png" alt=""><figcaption></figcaption></figure>



기본 버킷의 vBucket 수를 확인하세요.

모든 4개의 데이터 서비스 노드(노드 1, 2, 4, 마지막으로 3)에 대해 cbstats를 실행하세요.

```
[ec2-user@appserver ~]$ cbstats $NODE1:11210 all -u Administrator -p couchbase -b default | grep active_num
```

출력:

```
 vb_active_num:                                         384
 vb_active_num_non_resident:                            0
```

\


```bash
[ec2-user@appserver ~]$ cbstats $NODE2:11210 all -u Administrator -p couchbase -b default | grep active_num
```

출력:

```
vb_active_num:                                         384
vb_active_num_non_resident:                            0
```



```bash
[ec2-user@appserver ~]$ cbstats $NODE4:11210 all -u Administrator -p couchbase -b default | grep active_num
```

출력:

```
 vb_active_num:                                         256
 vb_active_num_non_resident:                            0
```



```
[ec2-user@appserver ~]$ cbstats $NODE1:11210 all -u Administrator -p couchbase -b default | grep replica_num
```

출력:

```
 vb_replica_num:                                        128
 vb_replica_num_non_resident:                           0
```



```bash
[ec2-user@appserver ~]$ cbstats $NODE2:11210 all -u Administrator -p couchbase -b default | grep replica_num
```

출력:

```
 vb_replica_num:                                        128
 vb_replica_num_non_resident:                           0
```



```bash
[ec2-user@appserver ~]$ cbstats $NODE4:11210 all -u Administrator -p couchbase -b default | grep replica_num
```

출력:

```
 vb_replica_num:                                        256
 vb_replica_num_non_resident:                           0
```





```bash
[ec2-user@appserver ~]$ cbstats $NODE3:11210 all -u Administrator -p couchbase -b default | grep active_num
```

출력:

```
No response.

Note: Couch03 couchservice is dead
```



계산해 보세요… 활성 vBucket은 몇 개입니까? (1024)

복제 vBucket은 몇 개입니까? (512)\




## 2. 삽입된 데이터를 읽기

우리가 삽입한 10개의 키를 읽는 다음 명령어를 실행하세요.

```bash
[ec2-user@AppServer ~]$ cbc-cat key-1 key-2 key-3 key-4 key-5 key-6 key-7 key-8 key-9 key-10
```

출력:

```
key-1                CAS=0x158dbe1058080000, Flags=0x0, Size=7, Datatype=0x00
value-1
key-3                CAS=0x158dbe323e320000, Flags=0x0, Size=7, Datatype=0x00
value-3
key-6                CAS=0x158dbe4065800000, Flags=0x0, Size=7, Datatype=0x00
value-6
key-4                CAS=0x158dbe37fe890000, Flags=0x0, Size=7, Datatype=0x00
value-4
key-5                CAS=0x158dbe3cb8ce0000, Flags=0x0, Size=7, Datatype=0x00
value-5
key-9                CAS=0x158dbe4c2b3f0000, Flags=0x0, Size=7, Datatype=0x00
value-9
key-10               CAS=0x158dbe4f7deb0000, Flags=0x0, Size=8, Datatype=0x00
value-10
key-2                CAS=0x158dbe2f32620000, Flags=0x0, Size=7, Datatype=0x00
value-2
key-7                CAS=0x158dbe43625d0000, Flags=0x0, Size=7, Datatype=0x00
value-7
key-8                CAS=0x158dbe46c9f60000, Flags=0x0, Size=7, Datatype=0x00
value-8
```

이제 모든 10개의 키가 반환되는 것을 확인했나요?

{% hint style="info" %}
이제 모든 10개의 키를 읽을 수 있지만, 다른 노드에 두 번째 장애가 발생하면 백업이 없습니다.
{% endhint %}





## 3. 클러스터 리밸런스

이제 리밸런스가 필요합니다. Rebalance를 클릭하세요.

<figure><img src="../.gitbook/assets/image (2).png" alt=""><figcaption></figcaption></figure>

리밸런스가 이제 시작되며 완료까지 약 2\~3분 정도 소요됩니다. 완료 후 클러스터에는 5개의 노드만 표시될 것입니다.

<figure><img src="../.gitbook/assets/image (3).png" alt=""><figcaption></figcaption></figure>

리밸런스가 완료된 후, Node #3에 있던 활성 키(key-4와 key-6, 제 환경 기준)를 조회해 보세요. 복제본이 활성화되었기 때문에 다시 정상적으로 조회되는 것을 확인할 수 있을 것입니다.

```bash
[ec2-user@AppServer~]$ cbc-cat –u Administrator –P couchbase -U couchbase://ec2-54-172-130-66.compute-1.amazonaws.com/default key-4
```

출력:

```
key-7                CAS=0x1da067b2e1920400, Flags=0x0, Size=7
value-7
```

\
key-8을 읽는 것도 제 클러스터에서는 정상적으로 동작할 것입니다. 이 키 역시 Node #3에 있었지만 이제 복제본이 활성화되었기 때문입니다.

```bash
[ec2-user@ AppServer ~]$ cbc-cat –u Administrator –P couchbase -U couchbase://ec2-54-172-130-66.compute-1.amazonaws.com/default key-6
```

\
출력:

```
key-8                CAS=0x713bce5b3e930400, Flags=0x0, Size=7
value-8
```

제가 이전에 만든 차트를 보면, key-4와 key-6은 데이터 복제본을 보유하고 있던 노드이기 때문에 Node #2에서 제공되고 있다고 추정할 수 있습니다. 아래는 제 환경에서 다시 출력한 차트입니다.

| Key    | <p>Active<br>Node #</p> | <p>Replica<br>Node #</p> |
| ------ | ----------------------- | ------------------------ |
| key-1  | 4                       | 2                        |
| key-2  | 4                       | 2                        |
| key-3  | 1                       | 3                        |
| key-4  | 2                       | 3                        |
| key-5  | 2                       | 4                        |
| key-6  | 2                       | 3                        |
| key-7  | 3                       | 2                        |
| key-8  | 3                       | 2                        |
| key-9  | 1                       | 4                        |
| key-10 | 1                       | 4                        |



여러분의 환경에 맞는 차트를 확인하고, 원래 Node #3에 있던 활성 키의 복제본을 어떤 노드가 호스팅하고 있는지 파악해 보세요.

이제 어떤 노드가 해당 키들을 호스팅하고 있는지 확인해 보세요(저는 key-4와 key-6을 직접 확인할 예정입니다).

```bash
[ec2-user@ AppServer ~]$ cbc-hash  key-4 key-6
```

출력:

```
key-4: [vBucket=646, Index=2] Server: ec2-18-144-49-199.us-west-1.compute.amazonaws.com:11210, CouchAPI: http://ec2-18-144-49-199.us-west-1.compute.amazonaws.com:8092/default
Replica #0: Index=0, Host=ec2-13-56-188-91.us-west-1.compute.amazonaws.com:11210

key-6: [vBucket=136, Index=0] Server: ec2-13-56-188-91.us-west-1.compute.amazonaws.com:11210, CouchAPI: http://ec2-13-56-188-91.us-west-1.compute.amazonaws.com:8092/default
Replica #0: Index=1, Host=ec2-13-56-209-124.us-west-1.compute.amazonaws.com:11210
```

제 클러스터의 경우, 위 출력 결과는 다음과 같습니다:

| Key   | <p>Active<br>Node #</p> | <p>Replica<br>Node #</p> |
| ----- | ----------------------- | ------------------------ |
| key-4 | 2                       | 1                        |
| key-6 | 2                       | 1                        |

이 시점에서 Node #2가 활성 복제본을 호스팅하고 있는 것은 자연스러운 일입니다.

key-4와 key-6의 복제본이 활성 데이터와 동일한 서버 그룹에 호스팅될 수 있다는 점이 다소 이상하게 보일 수도 있습니다(제 환경에서는 둘 다 Group #1에 있지만, 여러분의 환경에서는 다를 수 있습니다). 이러한 상황은 각 서버 그룹에 노드 수가 균등하지 않은 아주 작은 Couchbase 클러스터에서 발생할 수 있습니다. Couchbase의 모범 사례는 서버 그룹 내에서 장애가 발생한 하드웨어 노드를 작동 중인 노드로 교체한 다음 전체 노드를 대상으로 리밸런스를 실행하는 것입니다.

위의 cbc-hash 명령에서 key-4의 활성 사본은 현재 공용 호스트 이름이 ec2-18-144-49-199.us-west로 시작하는 서버의 vBucket 646에 있으며, 이는 제 환경에서 Node #4에 해당합니다(여러분의 실습 환경에서는 다른 서버일 수도 있습니다).\


## 4. 데이터 파일에서 키 찾기

Couchbase 클러스터에서 key-4를 포함하고 있는 데이터 파일을 찾아봅시다. 이전 섹션에서 어떤 키(원래 Node #3에서 활성 상태였던 키)로 작업했든 상관없이, 이번 섹션에서는 key-4를 찾아보겠습니다.

클러스터의 어떤 노드의 공용 호스트 이름이든 상관없이 App Server(검은색 VM)에서 다음 cbc hash 명령을 실행하여 key-4가 활성화된 위치를 확인하세요.

```bash
[ec2-user@ AppServer ~]$ cbc-hash  key-4
```

출력:

```
key-4: [vBucket=646, Index=2] Server: ec2-18-144-49-199.us-west-1.compute.amazonaws.com:11210, CouchAPI: http://ec2-18-144-49-199.us-west-1.compute.amazonaws.com:8092/default
Replica #0: Index=0, Host=ec2-13-56-188-91.us-west-1.compute.amazonaws.com:11210
```

\
key-4가 제 환경에서는 Node #4의 vBucket 646에 있는 것으로 보이므로, 저는 Node #4(Couchbase04/연한 파란색 PuTTY 창)의 PuTTY 창으로 전환해 key-4가 실제로 Node #4에 있는지 확인하겠습니다.

{% hint style="info" %}
여러분의 실습 환경에서는 key-4의 활성 사본이 위치한 노드로 전환하세요. 반드시 Node #4일 필요는 없습니다!
{% endhint %}

이제 key-4가 있는 노드로 이동해 실제로 vBucket 데이터 파일에서 이를 찾아봅시다.

\
먼저 루트 사용자로 전환하세요:

```bash
[ec2-user@Couchbase02 ~]$ sudo –s
```

\
기본 버킷의 데이터 파일이 있는 디렉터리로 이동하세요:

```bash
[root@Couchbase02 ec2-user] cd /opt/couchbase/var/lib/couchbase/data/default/
```

vBucket #646을 검색하세요 (여러분의 환경에서는 다른 번호의 버킷일 수도 있습니다!):

```bash
[root@Couchbase02 default] ls | grep 646
```

출력:

```
646.couch.1
```

해당 버킷을 찾았으면 couch\_dbdump 명령어를 사용해 그 내용을 출력하세요.

```bash
[root@Couchbase02 default]  /opt/couchbase/bin/couch_dbdump ./646.couch.1
```

출력:

```
Dumping "./646.couch.1":
Doc seq: 1
     id: key-4
     rev: 1
     content_meta: 131
     size (on disk): 17
     cas: 1553106594215559168, expiry: 0, flags: 0, datatype: 0x00 (raw)
     size: 7
     data: (snappy) value-4

Total docs: 1
```

\
루트(root)에서 로그아웃하세요:

```bash
[root@Couchbase02 default] exit
```

훌륭합니다! 기본 데이터 파일에서 key-4를 성공적으로 찾았습니다.\


## 5. 리밸런스 이후 활성/복제 vBucket 수 확인

다음으로, 1번, 2번, 4번 노드의 공용 호스트 이름에 대해 cbstats 명령어를 실행하여 현재 몇 개의 vBucket이 할당되어 있는지 확인하세요. (이 명령어는 App Server/검은색 노드에서 실행해야 합니다.)

```bash
[ec2-user@appserver ~]$ cbstats $NODE1:11210 all -u Administrator -p couchbase -b default | grep active_num
```

\
출력:

```
 vb_active_num:                                         341
 vb_active_num_non_resident:                            0
```





```bash
[ec2-user@appserver ~]$ cbstats $NODE2:11210 all -u Administrator -p couchbase -b default | grep active_num
```

출력:

```
 vb_active_num:                                         341
 vb_active_num_non_resident:                            0
```

\


```bash
[ec2-user@appserver ~]$ cbstats $NODE4:11210 all -u Administrator -p couchbase -b default | grep active_num
```

출력:

```
vb_active_num:                                         342
vb_active_num_non_resident:                            0
```

\
활성 vBucket 수를 확인한 후, 복제본(replica) 수를 확인하세요.

```
[ec2-user@appserver ~]$ cbstats $NODE1:11210 all -u Administrator -p couchbase -b default | grep replica_num
```

\
출력:

```
 vb_replica_num:                                        342
 vb_replica_num_non_resident:                           0
```



```bash
[ec2-user@appserver ~]$ cbstats $NODE2:11210 all -u Administrator -p couchbase -b default | grep replica_num
```

출력:

```
 vb_replica_num:                                        341
 vb_replica_num_non_resident:                           0
```



\


```bash
[ec2-user@appserver ~]$ cbstats $NODE4:11210 all -u Administrator -p couchbase -b default | grep replica_num
```

출력

```
 vb_replica_num:                                        341
 vb_replica_num_non_resident:                           0
```

클러스터의 첫 번째 노드는 이제 약 342개의 활성 vBucket과 341개의 복제 vBucket을 가지고 있습니다. 이렇게 되는 이유는 342 vBucket × 3 노드 = 1,026이기 때문입니다. Couchbase에는 총 1,024개의 vBucket이 있지만, 노드 수가 홀수일 경우 Couchbase는 vBucket을 최대한 균등하게 분배하려고 합니다.

이 경우, 나머지 두 노드는 각각 341개의 활성 vBucket을 가지게 되므로 342 + 341 + 341 = 1,024가 됩니다. 여러분의 환경에서 첫 번째 노드에서 보이는 vBucket 수는 약간 다를 수 있습니다.\


