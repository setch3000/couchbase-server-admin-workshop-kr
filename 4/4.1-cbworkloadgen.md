# 4.1 cbworkloadgen 유틸리티로 클러스터 벤치마크

{% hint style="info" %}
클러스터에서 Autofailover를 해제할 예정입니다(다음 단계에서).
{% endhint %}



## 1. 단일 스레드 벤치마크

이제 6노드 클러스터가 설정되었으니, App Server에서 6노드 클러스터로 100,000개의 아이템을 써보겠습니다.

검은색 배경의 AppServer VM PuTTY/터미널 셸로 전환합니다.

\
다음 명령어(cbworkloadgen)는 10바이트 크기의 아이템 100,000개를 삽입하며, 워크로드의 95%는 쓰기로 설정됩니다. 이는 1노드 Couchbase 클러스터에서 AppServer를 사용해 실행했던 동일한 테스트이며, 그때는 초당 약 151,400건의 작업이 처리되었습니다(클라우드 상태에 따라 결과가 달라질 수 있습니다).

다음 명령어에서 IP 주소를 첫 번째 VM의 퍼블릭 호스트 이름으로 변경하십시오.



```bash
[ec2-user@AppServer ~]$ cbworkloadgen -n $NODE1:8091 -i 100000 -r .95 -s 10 -u Administrator -p couchbase
```



```bash
[####################] 100.0% (105264/estimated 105263 msgs)
bucket: default, msgs transferred...
       :                total |       last |    per sec
 byte  :              1052640 |    1052640 |   202401.9
done
```

\
위 출력에서 클러스터가 초당 202,401.9바이트의 I/O를 처리하고 있음을 알 수 있습니다. 실제 숫자는 다를 수 있습니다. 동일한 명령어 끝에 -v 옵션을 추가해 자세한 출력(verbose 모드) 으로 실행해 보십시오.

```bash
[ec2-user@AppServer ~]$ cbworkloadgen -n $NODE1:8091 -i 100000 -r .95 -s 10 -v -u Administrator -p couchbase
```



```
2019-03-18 21:57:29,967: mt cbworkloadgen...
2019-03-18 21:57:29,967: mt  source : gen:json=0,max-items=100000,low-compression=False,prefix=pymc,ratio-sets=0.95,exit-after-creates=1,min-value-size=10
2019-03-18 21:57:29,967: mt  sink   : http://ec2-13-56-188-91.us-west-1.compute.amazonaws.com:8091
2019-03-18 21:57:29,967: mt  opts   : {'username': '<xxx>', 'destination_vbucket_state': 'active', 'verbose': 1, 'extra': {'max_retry': 10.0, 'rehash': 0.0, 'dcp_consumer_queue_length': 1000.0, 'data_only': 0.0, 'uncompress': 0.0, 'nmv_retry': 1.0, 'conflict_resolve': 1.0, 'cbb_max_mb': 100000.0, 'report': 5.0, 'mcd_compatible': 1.0, 'try_xwm': 1.0, 'backoff_cap': 0.1, 'batch_max_bytes': 400000.0, 'report_full': 2000.0, 'flow_control': 1.0, 'batch_max_size': 1000.0, 'seqno': 0.0, 'design_doc_only': 0.0, 'allow_recovery_vb_remap': 0.0, 'recv_min_bytes': 4096.0}, 'collection': None, 'ssl': False, 'threads': 1, 'key': None, 'password': '<xxx>', 'id': None, 'destination_operation': None, 'source_vbucket_state': 'active', 'silent': False, 'dry_run': False, 'single_node': False, 'bucket_destination': 'default', 'vbucket_list': None, 'separator': '::', 'bucket_source': None}
2019-03-18 21:57:29,975: mt Starting new HTTP connection (1): ec2-13-56-188-91.us-west-1.compute.amazonaws.com
2019-03-18 21:57:30,016: mt bucket: default
2019-03-18 21:57:35,029: w0   source : gen:json=0,max-items=100000,low-compression=False,prefix=pymc,ratio-sets=0.95,exit-after-creates=1,min-value-size=10(default@N/A-0)
2019-03-18 21:57:35,030: w0   sink   : http://ec2-13-56-188-91.us-west-1.compute.amazonaws.com:8091(default@N/A-0)
2019-03-18 21:57:35,030: w0          :                total |       last |    per sec
2019-03-18 21:57:35,030: w0    batch :                  106 |        106 |       21.1
2019-03-18 21:57:35,030: w0    byte  :              1052640 |    1052640 |   209953.4
2019-03-18 21:57:35,030: w0    msg   :               105264 |     105264 |    20995.3
  [####################] 100.0% (105264/estimated 105263 msgs)
bucket: default, msgs transferred...
       :                total |       last |    per sec
 batch :                  106 |        106 |       20.3
 byte  :              1052640 |    1052640 |   201818.0
 msg   :               105264 |     105264 |    20181.8
done
```



위 출력의 마지막 부분에서 App Server가 초당 20,181개의 메시지를 클러스터로 보내 총 약 105,264개의 메시지를 전송했음을 확인할 수 있습니다.

또한 100% 쓰기 전용으로 테스트를 다시 수행할 수도 있습니다(여기서 -r 인수를 1로 변경한 것에 주목하십시오).\


```bash
[ec2-user@AppServer ~]$ cbworkloadgen -n $NODE1:8091 -u Administrator -p couchbase -i 100000 -r 1 -s 10 -v
```

```bash
<output truncated>
[####################] 100.0% (100000/estimated 100000 msgs)
bucket: default, msgs transferred...
       :                total |       last |    per sec
 batch :                  100 |        100 |       19.2
 byte  :              1000000 |    1000000 |   192240.4
 msg   :               100000 |     100000 |    19224.0
done
```

\
순수하게 쓰기 전용 워크로드를 실행할 때, 클러스터는 초당 약 19,224건의 쓰기 작업을 처리합니다.

{% hint style="info" %}
이는 vCPU 수에 따라 달라질 수 있습니다.
{% endhint %}





## 2. Multithreaded benchmark <a href="#id-2_multithreaded_benchmark" id="id-2_multithreaded_benchmark"></a>

App Server에서 위와 동일한 명령어를 실행하되, 스레드 수를 10개(-t 10 인수 사용) 로 늘려 성능이 향상되는지 확인해 보십시오.

```
[ec2-user@AppServer ~]$ cbworkloadgen -n $NODE1:8091 -i 100000 -r 1 -s 10 -t 10 -v -u Administrator -p couchbase
```



```bash
2015-08-06 16:49:34,469: mt cbworkloadgen...
<output truncated>
2015-08-06 16:50:14,478: w0   sink   : http://ec2-52-6-74-39.compute-1.amazonaws.com:8091(default@N/A-1)
<output truncated>
2015-08-06 16:50:14,744: w2   source : gen:json=0,max-items=100000,low-compression=False,prefix=pymc,ratio-sets=1.0,exit-after-creates=1,min-value-size=10(default@N/A-2)
2015-08-06 16:50:14,744: w2   sink   : http://ec2-52-6-74-39.compute-1.amazonaws.com:8091(default@N/A-2)
2015-08-06 16:50:14,745: w2          :                total |       last |    per sec
2015-08-06 16:50:14,746: w2    batch :                  100 |        100 |        2.5
2015-08-06 16:50:14,746: w2    byte  :              1000000 |    1000000 |    24863.1
2015-08-06 16:50:14,746: w2    msg   :               100000 |     100000 |     2486.3
2015-08-06 16:50:14,780: w5   source : gen:json=0,max-items=100000,low-compression=False,prefix=pymc,ratio-sets=1.0,exit-after-creates=1,min-value-size=10(default@N/A-6)
2015-08-06 16:50:14,781: w5   sink   : http://ec2-52-6-74-39.compute-1.amazonaws.com:8091(default@N/A-6)
2015-08-06 16:50:14,781: w5          :                total |       last |    per sec
2015-08-06 16:50:14,781: w5    batch :                  100 |        100 |        2.5
2015-08-06 16:50:14,781: w5    byte  :              1000000 |    1000000 |    24843.1
2015-08-06 16:50:14,782: w5    msg   :               100000 |     100000 |     2484.3
2015-08-06 16:50:14,818: w4   source : gen:json=0,max-items=100000,low-compression=False,prefix=pymc,ratio-sets=1.0,exit-after-creates=1,min-value-size=10(default@N/A-4)
<output truncated>
-size=10(default@N/A-0)
2015-08-06 16:50:14,907: w1   sink   : http://ec2-52-6-74-39.compute-
<output truncated>
2015-08-06 16:50:14,910: w9   source : gen:json=0,max-items=100000,low-compression=False,prefix=pymc,ratio-sets=1.0,exit-after-creates=1,min-value-size=10(default@N/A-5)
2015-08-06 16:50:14,910: w9   sink   : http://ec2-52-6-74-39.compute-1.amazonaws.com:8091(default@N/A-5)
2015-08-06 16:50:14,910: w9          :                total |       last |    per sec
2015-08-06 16:50:14,910: w9    batch :                  100 |        100 |        2.5
2015-08-06 16:50:14,911: w9    byte  :              1000000 |    1000000 |    24763.8
2015-08-06 16:50:14,911: w9    msg   :               100000 |     100000 |     2476.4
  [####################] 99.9% (999000/estimated 1000000 msgs)
bucket: default, msgs transferred...
       :                total |       last |    per sec
 batch :                 1000 |       1000 |       24.4
 byte  :             10000000 |   10000000 |   244138.6
 msg   :              1000000 |    1000000 |    24413.9
done
```



위 출력은 각 스레드의 통계를 개별적으로 보여주므로, 예시에서는 2번째(w2)와 5번째(w5) 스레드의 결과를 확인할 수 있습니다.\


그러나 전체 쓰기 처리량은 초당 24,413.9건으로 실제로 감소했습니다. 즉, 스레드 수를 일정 수준 이상 늘리는 것은 성능 향상에 도움이 되지 않고 오히려 방해가 될 수 있습니다.

\


## 3. AppServer 확인

아래 스크린샷은 Amazon CloudWatch 대시보드에서 가져온 것으로, 10개의 쓰기 스레드를 실행하는 위 명령어가 동작 중일 때 App Server의 CPU 사용량을 보여줍니다(그래프의 빨간색 원 표시 부분 참조).

(학생들은 CloudWatch 메트릭에 접근할 수 없지만, 성능 실습에서는 유사한 메트릭을 수집하기 위해 다른 도구를 사용할 예정입니다.)

<figure><img src="../.gitbook/assets/image (82).png" alt=""><figcaption></figcaption></figure>



위에서 보듯이, 10개의 스레드가 생성될 때 App Server의 CPU 사용량이 거의 100%에 도달합니다.

다음 CloudWatch 스크린샷은 10개 스레드 쓰기 테스트 동안 App Server에서 발생한 네트워크 출력 트래픽(Network out) 을 바이트 단위로 보여줍니다. 피크 시점에서 App Server는 클러스터로 40,000,000바이트, 즉 305메가비트(38메가바이트) 의 트래픽을 전송했습니다.

<figure><img src="../.gitbook/assets/image (83).png" alt=""><figcaption></figcaption></figure>



## 4. 클러스터 노드에 부하가 고르게 분산되는가?

초당 약 13,000\~14,000건의 쓰기가 4개의 노드에 어떻게 분산되고 있는지 확인할 방법이 있을까요? 일부 서버에 쓰기 요청이 편향되어 특정 노드가 더 많은 부하를 처리하는지 조사하는 것이 좋습니다.

우선 App Server에서 6노드 클러스터로 100% 쓰기 작업을 연속적으로 실행합니다. 스레드는 2개만 사용하고, 각 아이템 크기는 10바이트로 설정합니다. 명령어의 -l 옵션은 사용자가 중단할 때까지 워크로드 생성기가 무한 루프로 실행되도록 합니다.



```bash
[ec2-user@AppServer ~]$ cbworkloadgen -n $NODE1:8091 -u Administrator -p couchbase -i 100000 -r 1 -s 10 -t 2 -v -l
```



위 명령어가 루프에서 실행되는 동안, Couchbase Web UI로 전환하여 통계를 확인하고 질문에 대한 답을 찾아봅니다.

Dashboard 링크를 클릭한 후 드롭다운에서 Cluster overview를 선택하고, 다른 드롭다운에서 default 버킷, 그리고 All server nodes(6) 를 선택합니다.

왼쪽에 있는 default 버킷 차트를 확인합니다.



<figure><img src="../.gitbook/assets/image (84).png" alt=""><figcaption></figcaption></figure>



통계 페이지에서 All Server Nodes의 default 데이터 버킷을 보면 초당 약 20,000\~30,000건의 작업이 표시되어야 합니다.

아래 스크린샷에서는 한 노드만 선택되어 전체 작업의 1/4만 표시되고 있습니다.

<figure><img src="../.gitbook/assets/image (85).png" alt=""><figcaption></figcaption></figure>



Couchbase Server 7에서 통계 영역이 변경되었습니다. 이제 통계는 화면 왼쪽 상단 링크에 있는 Dashboard의 일부로 표시됩니다.

왼쪽 드롭다운 메뉴에서 익숙해져야 할 두 가지 보기 옵션이 있습니다:

* Cluster Overview
* All Services



<figure><img src="../.gitbook/assets/image (86).png" alt=""><figcaption></figcaption></figure>



마우스 커서를 통계 상자 위에 올리면 모든 메트릭 설명이 표시됩니다.



<figure><img src="../.gitbook/assets/image (87).png" alt=""><figcaption></figcaption></figure>



왼쪽 드롭다운 메뉴에서 All Services를 선택합니다.

data total ram size는 초당 약 40MB 속도로 30,000개의 아이템을 반복적으로 삽입하고 있음에도 불구하고 47.7MB로 비교적 낮게 표시됩니다. 그 이유는 워크로드 생성기가 10바이트 크기의 100,000개 키를 새로운 키가 아니라 동일한 키에 반복해서 덮어쓰고 있기 때문입니다. 참고로 10바이트 × 100,000개의 키는 고작 976킬로바이트에 불과합니다.

이 페이지를 위로 스크롤한 뒤, 클러스터의 4개 데이터 서비스 노드 중 하나를 선택하여 개별화된 메트릭을 확인합니다.

<figure><img src="../.gitbook/assets/image (88).png" alt=""><figcaption></figcaption></figure>

4개 노드 각각에서 대략 6,000-8,000개의 쓰기 작업이 수행되는 것을 확인할 수 있어야 합니다. 초당 6-8,000 ops에 4개 노드를 곱하면 대략 **초당 30,000 ops**와 같습니다.

드롭다운에서 위 단계를 반복하여 다른 3개 노드도 초당 약 6,000-8,000 ops를 처리하고 있는지 확인하세요:

> _다른 3개 노드의 스크린샷은 실습에서 표시되지 않음_





## 5. 클러스터가 이미 포화 상태인가, 아니면 더 많은 작업을 초당 처리할 수 있는가?

클러스터의 첫 번째 노드(Couchbase01)로 전환하여 워크로드 생성기를 루프로 실행합니다.\
\
첫 번째 Couchbase 클러스터 노드를 듀얼 역할로 사용해 클러스터에 더 많은 쓰기를 발생시키고, 얼마나 더 많은 쓰기를 처리할 수 있는지 확인해 보겠습니다. 현재 병목 현상은 클라우드에 있는 App Server 자체일 수 있으며, 이 서버의 NIC가 초당 30,000개의 아이템 이상을 처리하지 못할 수도 있습니다.

App Server에서 루프로 실행했던 것과 동일한 매개변수를 사용해 첫 번째 노드에서 워크로드 생성기를 실행합니다.



```bash
[ec2-user@Couchbase01 ~]$  cbworkloadgen -n $NODE1:8091 -u Administrator -p couchbase -i 100000 -r 1 -s 10 -t 2 -v –l
```

Web UI에서 이제 클러스터가 4개의 데이터 서비스 노드 전반에 걸쳐 초당 약 30,000\~50,000건의 쓰기 작업을 처리하고 있는 것을 확인할 수 있습니다. 또한 가끔 default 버킷이 압축(compacted) 되고 있다는 메시지가 표시될 수도 있습니다.

<figure><img src="../.gitbook/assets/image (90).png" alt=""><figcaption></figcaption></figure>

Anyway, the `Summary` section will show an exact count such as **\~50.2 ops per second**:

<figure><img src="../.gitbook/assets/image (92).png" alt=""><figcaption></figcaption></figure>

두 대의 머신을 사용해 쓰기 작업을 실행해 보면, App Server 한 대만으로는 클러스터가 처리할 수 있는 트래픽을 포화시키기에는 역부족이라는 것을 알 수 있습니다.

이제 첫 번째 Couchbase 노드(Couchbase01 다크 블루 VM)로 돌아가 CTRL+C를 눌러 cbworkloadgen 명령어를 중지합니다.

그러면 다음과 같은 메시지가 표시되고 명령줄로 돌아가게 됩니다:

```
....................................^Cinterrupted.
[ec2-user@Couchbase01 ~]$
```



Web UI에서 클러스터의 쓰기 작업이 초당 20\~30K 수준으로 다시 줄어든 것을 확인할 수 있습니다.

<figure><img src="../.gitbook/assets/image (93).png" alt=""><figcaption></figcaption></figure>

