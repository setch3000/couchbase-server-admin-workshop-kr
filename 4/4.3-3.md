# 4.3 클러스터에서 노드 #3 제거

클러스터의 노드가 데이터를 제공할 수 없는 경우, 해당 노드를 장애 조치(Failover) 할 수 있습니다. 장애 조치란 Couchbase Server가 클러스터에서 해당 노드를 제거하고, 다른 노드에 있는 복제 데이터를 클라이언트 요청에 사용할 수 있도록 하는 것을 의미합니다. Couchbase에서 충분한 수의 복제본을 구성해두면, 클러스터는 하나 이상의 노드 장애가 발생하더라도 저장된 데이터에 대한 액세스에 영향을 주지 않고 이를 처리할 수 있습니다. 장애 조치는 Web UI, REST API 또는 Couchbase CLI를 통해 수동으로 시작할 수 있습니다.

또한 Couchbase Server를 구성하여 장애가 발생한 노드를 클러스터에서 자동으로 제거하고 클러스터가 축소된 모드로 작동하도록 할 수도 있습니다. 그러나 이 자동 옵션을 사용하면, 클러스터에 남아 있는 정상 노드들의 워크로드가 증가하게 됩니다. 여전히 노드 장애를 해결하고, 정상 노드를 클러스터에 복구한 후, 클러스터를 리밸런스 해야 장애 발생 전 상태로 돌아갈 수 있습니다. 어떤 분산 시스템에서든 장애를 자동으로 처리하도록 구성하면 예기치 못한 문제가 발생할 수 있습니다. 장애 원인을 파악하지 못하거나 남아 있는 시스템에 가해질 부하를 이해하지 못한다면, 자동 장애 조치는 문제를 해결하기보다는 더 많은 문제를 일으킬 수 있습니다.

정상적으로 작동 중인 노드를 장애 조치하려 하면 데이터 손실이 발생할 수도 있습니다. 장애 조치는 클러스터에서 해당 노드를 즉시 제거하고, 다른 노드에 복제되지 않았거나 디스크에 저장되지 않은 데이터는 영구적으로 손실될 수 있기 때문입니다.

장애 조치는 노드 제거(Remove)나 리밸런스(Rebalance)와는 다른 작업임을 유의해야 합니다. 일반적으로 정상 노드를 제거하는 것은 유지보수 등의 이유 때문이며, 장애 조치는 작동하지 않는 노드를 처리하기 위한 것입니다. 정상적인 노드를 관리 목적으로 제거하려면 장애 조치가 아니라 Remove와 Rebalance 기능을 사용해야 합니다.

실제 장애 조치를 실행하기 전에, 기본 버킷의 설정과 구성, 그리고 vBucket이 저장되는 위치 및 실습 클러스터에서 자동 장애 조치(autoFailover) 가 켜져 있는지 확인해 보겠습니다.



## 1.  노드 1의 vBucket 개수 확인

App 서버(Black VM)에서 1번 노드의 퍼블릭 호스트 이름을 사용하여 cbstats 명령어를 실행하고 active\_num을 grep으로 확인합니다:

```
[ec2-user@AppServer ~]$ cbstats $NODE1:11210 all -u Administrator -p couchbase -b default | grep active_num
```

```
 vb_active_num:                      256
 vb_active_num_non_resident:           0
```

이제 `replica_num`을 grep으로 확인해 보겠습니다:\


```
[ec2-user@AppServer ~]$ cbstats $NODE1:11210 all -u Administrator -p couchbase -b default | grep
replica_num
```

```
 vb_replica_num:                     0
 vb_replica_num_non_resident:        0
```

1번 노드는 256개의 활성 vBucket만 호스팅하고 있으며, 복제 vBucket은 없다는 것을 확인할 수 있습니다.

Couchbase의 각 버킷은 총 1024개의 개별 vBucket(가상 버킷)으로 구성됩니다. 이 1024개의 vBucket은 클러스터 전체에 고르게 분산되어 각 노드가 버킷의 부하를 공평하게 호스팅하도록 합니다.





## 2. 노드 2의 vBucket 개수 확인

2번 노드의 퍼블릭 호스트 이름(Cluster-IPs 스프레드시트에서 확인)을 사용해 동일한 명령어를 실행해 보십시오:

```
[ec2-user@AppServer ~]$ cbstats $NODE2:11210 all -u Administrator -p couchbase -b default | grep active_num
```

```
 vb_active_num:                                         256
 vb_active_num_non_resident:                            0
```

```
[ec2-user@appserver ~]$ cbstats $NODE2:11210 all -u Administrator -p couchbase -b default | grep replica_num
```

```
 vb_replica_num:                                        0
 vb_replica_num_non_resident:                           0
```

2번 노드에서도 active-num이 256으로 표시되는 것을 확인할 수 있습니다.

기본 버킷에 대해 복제본이 구성되지 않았기 때문에, 우리는 Rack Awareness/Server Groups 기능을 사용하여 복제 vBucket을 다른 서버 그룹에 분산하지 않고 있다는 점을 유의하세요.



## 3. 기본 버킷의 복제본 활성화

이제 Web UI로 전환하여 기본 버킷에 대해 1개의 복제본을 활성화해 보겠습니다. 하지만 먼저 기본 버킷을 플러시(Flush)하여 안에 있는 모든 데이터를 삭제해야 합니다.

Web UI 왼쪽 메뉴에서 Buckets를 클릭하고, 버킷의 Items 개수를 확인한 뒤(스크린샷의 숫자와는 다를 수 있음), Flush를 클릭합니다.

<figure><img src="../.gitbook/assets/image (2) (1).png" alt=""><figcaption></figcaption></figure>

팝업에서도 `Flush`를 클릭합니다.

<div align="left"><figure><img src="../.gitbook/assets/image (3) (1).png" alt=""><figcaption></figcaption></figure></div>

이제 항목 수가 0으로 표시되고, 이 버킷에 대해 복제본이 여전히 비활성화되어 있는 것을 확인할 수 있습니다.

<figure><img src="../.gitbook/assets/image (94).png" alt=""><figcaption></figcaption></figure>

다음으로 기본 버킷에 대해 하나의 복제본을 활성화합니다. 다시 \*\*편집(Edit)\*\*을 클릭합니다.

`고급 버킷 설정(Advanced bucket settings)`을 확장하면 Replicas 설정을 볼 수 있습니다. Enable 옆에 체크 표시를 하고, Number of replica copies를 1로 설정합니다.

<figure><img src="../.gitbook/assets/image (95).png" alt=""><figcaption></figcaption></figure>

`Save Changes` 를 클릭합니다. 이제 1개의 복제본이 구성된 것을 확인할 수 있습니다.

<figure><img src="../.gitbook/assets/image (96).png" alt=""><figcaption></figcaption></figure>

Couchbase Web UI에서 Servers 페이지를 클릭합니다. 기본 버킷에 복제본을 추가했음에도 불구하고, 이번에는 재균형(Rebalance)이 필요한 특정 노드가 없다는 것을 확인할 수 있습니다. 이는 기본 버킷이 비어 있고, 이전에 비우기(Flush)를 했기 때문에 키가 0개이기 때문입니다. 그러나 주황색 막대에 메시지가 표시되는 것도 확인할 수 있습니다.

{% hint style="danger" %}
경고: 리밸런스 필요, 일부 데이터가 현재 복제되지 않았습니다.
{% endhint %}

<figure><img src="../.gitbook/assets/image (97).png" alt=""><figcaption></figcaption></figure>



## 아직 “리밸런스(Rebalance)“를 클릭하지 마세요…​



## 4. 활성/복제 vBucket 수 확인

App Server 명령줄로 전환합니다… 다음 몇 가지 명령어를 통해 현재 시점에서 재조정(rebalance)이 필요한 이유를 설명합니다.

App Server(검은색 VM)에서 다음 명령어 세트를 실행합니다: 다시 한 번, 1번 노드의 퍼블릭 호스트 이름에 대해 cbstats 명령을 실행하고 active\_num 및 replica\_num을 grep으로 검색합니다.



```
[ec2-user@AppServer ~]$ cbstats $NODE1:11210 all -u Administrator -p couchbase -b default | grep active_num
```

```
vb_active_num:                      256
vb_active_num_non_resident:           0
```

```
[ec2-user@AppServer ~]$ cbstats $NODE1:11210 all -u Administrator -p couchbase -b default | grep replica_num
```

```
vb_replica_num:                     0
vb_replica_num_non_resident:        0
```



1번 노드가 여전히 256개의 활성 vBucket만 호스팅하고 있고 복제 vBucket은 없다는 점에 주목하세요.

리밸런싱이 시작되기 전에 1번 노드에 대해 cbstats 명령어를 루프에서 실행해 보면, 이 노드의 복제 vBucket 수가 천천히 증가하여 최대 256개의 vBucket에 도달하는 것을 확인할 수 있습니다.

```
[ec2-user@AppServer ~]$  while true; do cbstats $NODE1:11210 all -u Administrator -p couchbase -b default | grep replica_num; sleep 2; done
```

### &#x20;<a href="#id-5_rebalance_the_cluster" id="id-5_rebalance_the_cluster"></a>

### 5. 클러스터 리밸런싱 <a href="#id-5_rebalance_the_cluster" id="id-5_rebalance_the_cluster"></a>

Couchbase Web UI로 돌아가서 Rebalance를 클릭하여 복제 vBucket을 생성하세요.

<figure><img src="../.gitbook/assets/image (99).png" alt=""><figcaption></figcaption></figure>



리밸런스 작업은 완료까지 약 1-2분 정도 소요됩니다.

<figure><img src="../.gitbook/assets/image (100).png" alt=""><figcaption></figcaption></figure>



루프에서 첫 번째 노드를 관찰하면 이 노드의 복제 vBucket 수가 천천히 증가하여 최대 256 vBucket에 도달하는 것을 볼 수 있습니다.

```bash
[ec2-user@AppServer ~]$  while true; do  cbstats $NODE1:11210 all -u Administrator -p couchbase -b default | grep replica_num; sleep 2; done
```

```bash
 vb_replica_num_non_resident:                           0
 vb_replica_num:                                        5
 vb_replica_num_non_resident:                           0
 vb_replica_num:                                        42
 vb_replica_num_non_resident:                           0
 vb_replica_num:                                        81
 vb_replica_num_non_resident:                           0
 vb_replica_num:                                        122
 vb_replica_num_non_resident:                           0
 vb_replica_num:                                        163
 vb_replica_num_non_resident:                           0
 vb_replica_num:                                        201
 vb_replica_num_non_resident:                           0
 vb_replica_num:                                        252
 vb_replica_num_non_resident:                           0
 vb_replica_num:                                        256
 vb_replica_num_non_resident:                           0
 vb_replica_num:                                        256
 vb_replica_num_non_resident:                           0
 vb_replica_num:                                        256
```

\
마지막으로 첫 번째 노드에서 `256개의 복제 vBucket`을 볼 수 있습니다.

Rebalance 작업이 완료되면 서버 페이지의 주황색 경고 메시지가 사라집니다. 또한 현재 데이터 노드 2개는 서버 그룹 1에, 데이터 노드 2개는 서버 그룹 2에 속해 있으며(쿼리 및 인덱스 서비스 노드는 제외) 있다는 점도 확인할 수 있습니다.

<figure><img src="../.gitbook/assets/image (101).png" alt=""><figcaption></figcaption></figure>





## 6. 기본 버킷에 10개의 항목 추가

실제로 장애 조치를 트리거하기 전에 기본 버킷에 10개의 키를 작성해 보겠습니다. 그런데 이 10개의 키는 4노드 클러스터의 활성 vBucket 내에서 어디에 분산될까요?

다음의 cbc hash 명령어 출력은 10줄이 표시되며, 각 줄은 쓰기 작업이 발생할 경우(다음 실습 단계에서 cbc-create 명령을 사용할 때) 클러스터의 4개 데이터 서비스 노드 중 어떤 노드로 키가 전송될지를 나타냅니다.

이 명령어는 App Server(검은색 VM)에서 1번 노드의 공용 IP를 대상으로 실행합니다. (사용자 ID, 비밀번호, IP 주소는 이전에 설정했으므로 여기서는 따로 지정할 필요가 없습니다.)

```bash
[ec2-user@AppServer ~]$ cbc-hash  key-1 key-2 key-3 key-4 key-5 key-6 key-7 key-8 key-9 key-10
```

```bash
key-1: [vBucket=748, Index=2] Server: ec2-54-172-130-69.compute-1.amazonaws.com:11210, CouchAPI: http://ec2-54-172-130-69.compute-1.amazonaws.com:8092/default
Replica #0: Index=1, Host=ec2-54-172-130-66.compute-1.amazonaws.com:11210

key-2: [vBucket=997, Index=3] Server: ec2-54-172-130-8.compute-1.amazonaws.com:11210, CouchAPI: http://ec2-54-172-130-8.compute-1.amazonaws.com:8092/default
Replica #0: Index=1, Host=ec2-54-172-130-66.compute-1.amazonaws.com:11210

key-3: [vBucket=226, Index=1] Server: ec2-54-172-130-66.compute-1.amazonaws.com:11210, CouchAPI: http://ec2-54-172-130-66.compute-1.amazonaws.com:8092/default
Replica #0: Index=3, Host=ec2-54-172-130-8.compute-1.amazonaws.com:11210

key-4: [vBucket=646, Index=2] Server: ec2-54-172-130-69.compute-1.amazonaws.com:11210, CouchAPI: http://ec2-54-172-130-69.compute-1.amazonaws.com:8092/default
Replica #0: Index=1, Host=ec2-54-172-130-66.compute-1.amazonaws.com:11210

key-5: [vBucket=385, Index=0] Server: ec2-54-172-130-15.compute-1.amazonaws.com:11210, CouchAPI: http://ec2-54-172-130-15.compute-1.amazonaws.com:8092/default
Replica #0: Index=3, Host=ec2-54-172-130-8.compute-1.amazonaws.com:11210

key-6: [vBucket=136, Index=1] Server: ec2-54-172-130-66.compute-1.amazonaws.com:11210, CouchAPI: http://ec2-54-172-130-66.compute-1.amazonaws.com:8092/default
Replica #0: Index=3, Host=ec2-54-172-130-8.compute-1.amazonaws.com:11210

key-7: [vBucket=911, Index=3] Server: ec2-54-172-130-8.compute-1.amazonaws.com:11210, CouchAPI: http://ec2-54-172-130-8.compute-1.amazonaws.com:8092/default
Replica #0: Index=1, Host=ec2-54-172-130-66.compute-1.amazonaws.com:11210

key-8: [vBucket=816, Index=3] Server: ec2-54-172-130-8.compute-1.amazonaws.com:11210, CouchAPI: http://ec2-54-172-130-8.compute-1.amazonaws.com:8092/default
Replica #0: Index=0, Host=ec2-54-172-130-15.compute-1.amazonaws.com:11210

key-9: [vBucket=55, Index=1] Server: ec2-54-172-130-66.compute-1.amazonaws.com:11210, CouchAPI: http://ec2-54-172-130-66.compute-1.amazonaws.com:8092/default
Replica #0: Index=2, Host=ec2-54-172-130-69.compute-1.amazonaws.com:11210

key-10: [vBucket=5, Index=1] Server: ec2-54-172-130-66.compute-1.amazonaws.com:11210, CouchAPI: http://ec2-54-172-130-66.compute-1.amazonaws.com:8092/default
Replica #0: Index=2, Host=ec2-54-172-130-69.compute-1.amazonaws.com:11210
```

위 명령어를 4개 노드 중 어느 노드의 공용 호스트명에 대해 실행하더라도 동일한 출력이 나온다는 점에 주의하세요.

\
서버 그룹이 구성되어 있으므로, 활성 데이터가 첫 번째 그룹(Node #1 또는 Node #2) 중 하나에 기록되면 해당 복제본은 두 번째 그룹(Node #3 또는 Node #4)에 저장됩니다.



다시 말해, 제 실습 환경에서는 서버 그룹이 다음과 같이 설정되어 있습니다:

* Server Group 1:
  * Node #1: ec2-54-85-43-128
  * Node #2: ec2-54-86-106-120
* Server Group 2:
  * Node #3: ec2-54-86-243-136
  * Node #4: ec2-54-85-206-193

\
따라서 Node #1 또는 Node #2에 기록되는 활성 데이터는 Node #3 또는 Node #4에 복제본이 저장되어야 합니다.

1\~2페이지 위로 올라가서 10개의 키가 저장될 위치를 보여주는 출력 결과를 보면 이 패턴이 더 명확해질 것입니다.

예를 들어, 제 환경에서는:

| Key    | <p>Active<br>Node #</p> | <p>Replica<br>Node #</p> |
| ------ | ----------------------- | ------------------------ |
| key-1  | 4                       | 2                        |
| key-2  | 4                       | 2                        |
| key-3  | **1**                   | 3                        |
| key-4  | 2                       | 3                        |
| key-5  | 2                       | 4                        |
| key-6  | **1**                   | 3                        |
| key-7  | 3                       | 2                        |
| key-8  | 3                       | 2                        |
| key-9  | **1**                   | 4                        |
| key-10 | **1**                   | 4                        |

위에서 Node #1이 4개의 활성 키를 보유하고 있고 복제 키는 0개라는 점에 주목하세요(“replica node” 열에는 Node 1에 대한 참조가 없습니다).

이 시점에서, 위에 있는 표와 유사한 표를 여러분의 클러스터 맞춤 설정을 기반으로 간단히 만들어 보시기 바랍니다. 여러분의 실습 환경에서도 활성 데이터의 복제본이 다른 서버 그룹에 배치되는 것을 확인할 수 있을 것입니다.

이제 App Server(검은색 VM)로 전환하여 클러스터에 10개의 키와 10개의 값을 삽입해 보겠습니다. 다음 두 페이지에 나오는 cbc create 및 cbc cat 명령어에서는 4개 노드 중 어느 노드의 공용 호스트명을 사용해도 됩니다(참고: 아래 명령 실행 시 적용되기까지 CTRL + D를 여러 번 눌러야 할 수도 있습니다).

```bash
[ec2-user@AppServer ~]$ for num in {1..10}; do cbc-create key-$num -V "{\"value\":\"${num}\"}"; done
```

```bash
key-1               Stored. CAS=0x15a3749fab37.1.0
key-2               Stored. CAS=0x15a3749fac0f0000
key-3               Stored. CAS=0x15a3749fac9a0000
key-4               Stored. CAS=0x15a3749fad6a0000
key-5               Stored. CAS=0x15a3749fae410000
key-6               Stored. CAS=0x15a3749faef30000
key-7               Stored. CAS=0x15a3749fafb10000
key-8               Stored. CAS=0x15a3749fb0640000
key-9               Stored. CAS=0x15a3749fb1210000
key-10              Stored. CAS=0x15a3749fb1e00000
```

웹 UI로 전환한 다음 왼쪽 메뉴에서 Buckets를 클릭하여 기본 버킷에 이제 10개의 항목이 있는지 확인하세요.

<figure><img src="../.gitbook/assets/image (102).png" alt=""><figcaption></figcaption></figure>

그런 다음 Documents를 클릭하고 드롭다운에서 default keyspace를 선택합니다.

다음 페이지에서 10개의 키가 사전식(lexicographical) 순서로 정렬되어 있는 것을 볼 수 있습니다. (페이지 보기를 10으로 변경)

<figure><img src="../.gitbook/assets/image (103).png" alt=""><figcaption></figcaption></figure>

App Server(검은색 VM)로 다시 전환한 뒤, REST API를 통해 기본 버킷의 Auto-failover 설정을 확인합니다. 이 명령에서는 4개 노드 중 어느 노드의 공용 호스트 이름이든 사용할 수 있습니다.

```bash
[ec2-user@AppServer ~]$ curl -u Administrator:couchbase -n http://$NODE1:8091/settings/autoFailover
```

```json
{
  "enabled":true,
  "timeout":120,
  "count":0,
  "failoverOnDataDiskIssues":
    {
      "enabled":false,
      "timePeriod":120
    },
  "failoverServerGroup":false,
  "maxCount":1
}
```

* enabled: auto-failover가 활성화되어 있으면 true, 비활성화되어 있으면 false
* timeout: 클러스터에서 auto-failover가 실행되기 전 경과해야 하는 초 단위 시간
* count: 0, 1, 2 또는 3으로 설정 가능. 클러스터 내에서 자동으로 failover될 수 있는 횟수를 나타냄. 한 번 auto-failover가 발생하면 count는 1로 설정됨
* maxCount: 1, 2 또는 3으로 설정 가능



## 7. Auto-Failover 비활성화

다음 명령은 auto-failover를 비활성화하고, 그 후 비활성화되었는지 확인합니다.

참고: 기본 설정은 활성화(enabled) 상태입니다.

```bash
[ec2-user@appserver ~]$ curl -u Administrator:couchbase -n http://$NODE1:8091/settings/autoFailover -i -d 'enabled=false'
```

```
HTTP/1.1 200 OK
X-XSS-Protection: 1; mode=block
X-Permitted-Cross-Domain-Policies: none
X-Frame-Options: DENY
X-Content-Type-Options: nosniff
Server: Couchbase Server
Pragma: no-cache
Expires: Thu, 01 Jan 1970 00:00:00 GMT
Date: Tue, 19 Mar 2019 17:12:47 GMT
Content-Length: 0
Cache-Control: no-cache,no-store,must-revalidate
```

자동 장애 조치 상태 확인:

```bash
[ec2-user@appserver ~]$ curl -u Administrator:couchbase -n http://$NODE1:8091/settings/autoFailover
```

```json
{
   "enabled":false,
   "timeout":120,
   "count":0,
   "failoverOnDataDiskIssues":{
      "enabled":false,
      "timePeriod":120
   },
   "failoverServerGroup":false,
   "maxCount":1
}
```

autoFailover 설정은 여기 설명된 대로 Couchbase CLI를 사용하여 구성할 수도 있습니다.

Web UI로 돌아가서 Servers를 클릭하세요.\


<figure><img src="../.gitbook/assets/image (104).png" alt=""><figcaption></figcaption></figure>

Items (Active/Replica) 열에서 우리가 생성한 10개의 키와 그에 대한 10개의 복제 키를 볼 수 있어야 하며, 따라서 총 개수는 20개가 됩니다. 여기서 표시되는 숫자는 앞서 이 랩에서 작성한 10개의 키에 대한 차트와 일치해야 합니다.

예를 들어, 제 클러스터에서 Node #1은 퍼블릭 호스트 이름이 ec2-54-85-43-128로 시작하며, 위 스크린샷에서 첫 번째 노드이기도 합니다. 이 노드는 4개의 활성 키와 0개의 복제 키를 보유하고 있습니다. 이는 앞서 작성한 차트에서 예상했던 결과와 일치합니다. 여러분의 환경에서도 몇 개의 노드에 대해 동일한 확인을 수행해 보세요.

다음으로 Server Group 2에 있는 두 노드에서 Couchbase 서비스를 중지할 것입니다. 아래 스크린샷을 보면, 제 클러스터에서는 이렇게 하면 6개의 활성 키를 잃게 됩니다(한 노드에서 4개, 다른 노드에서 2개).

<figure><img src="../.gitbook/assets/image (105).png" alt=""><figcaption></figcaption></figure>

좀 더 구체적으로, 제 설치 환경에서는 Node #3에서 active key-4와 key-6이 더 이상 사용 불가능해지고, Node #4에서는 active key-1과 key-2가 더 이상 사용 불가능해집니다.

Node 3과 Node 4에서 Couchbase 프로세스를 중지한 후에는 복제본이 활성화되고 해당 4개의 키를 활성화된 복제본에서 읽을 수 있도록, Node 3과 Node 4를 장애 조치(Failover)해야 합니다.



{% hint style="info" %}
귀하의 특정 설치 환경에서 테스트해야 할 키들은 제가 사용할 키들과는 다를 수 있습니다.

앞서 생성한 표를 활용해 Node 3과 Node 4에서 활성화된 키들을 파악하고, 이후 읽기 테스트 시 해당 키들을 사용하면 됩니다.

계속 진행하기 전에 Node 3에 있는 활성 키와 Node 4에 있는 활성 키를 미리 기록해 두는 것이 좋습니다.
{% endhint %}



Nodes #3와 #4에 있는 활성 키들을 기록하세요:\


* Node #3에 호스팅된 활성 키: \_\_
* Node #4에 호스팅된 활성 키: \_\_

\
먼저 Node #3에서 활성 키를 하나 읽습니다. 바로 위에서 작성한 양식을 사용해 Node #3의 키 중 하나를 선택하세요.

제가 사용한 실습 환경에서는 key-6을 선택했지만, 여러분의 환경에서는 다른 키를 선택해야 할 수도 있습니다.

아래 명령어를 App Server에서 실행하세요(아래 명령어에서 1번 노드의 공용 호스트 이름을 사용합니다):

```bash
[ec2-user@AppServer ~]$ cbc-cat –u Administrator –P couchbase -U couchbase://ec2-54-172-130-66.compute-1.amazonaws.com/default key-6
```

```
key-6                CAS=0x1da067b2e1920400, Flags=0x0, Datatype=0x0
value-6
```

지금까지는 순조롭게 진행되고 있습니다.



## 8. 노드 #3에서 Couchbase 서버 중지

다음으로 3번째 노드(Green VM/Couchbase03)의 PuTTY 창으로 전환하고 해당 노드에서 Couchbase 서비스를 중지합니다.

```bash
[ec2-user@Couchbase03 ~]$ sudo systemctl stop couchbase-server
```

```bash
Stopping couchbase-server
```

Couchbase 서버가 중지되었는지 확인합니다.

```bash
[ec2-user@Couchbase03 ~]$ sudo systemctl status couchbase-server
```

```
Aug 10 13:37:11 node3 systemd[1]: Stopped LSB: couchbase server.
```



## 9. 노드 #3에서 읽기를 시도합니다.

App Server(검은색 VM)로 전환하여 방금 노드 #3에서 읽었던 키를 다시 읽어보세요. 제 환경에서는 key-7을 다시 읽어보겠습니다. 이후의 모든 명령어는 별도의 안내가 없을 경우 App Server에서 실행됩니다.

```bash
[ec2-user@AppServer ~]$ cbc-cat -U $NODE3/default key-6
```

```bash
Failed to bootstrap instance. libcouchbase error: LCB_ECONNREFUSED (0x2C): The remote host refused the connection. Is the service up?
```

또는 명령어 기록(buffer)에서 이전에 실행한 cbc-cat 명령어를 다시 실행할 수도 있습니다.

이는 .cbcrc 파일에서 설정한 $NODE1 변수를 사용하고 있습니다.\


```bash
cbc-cat  key-1 key-2 key-3 key-4 key-5 key-6 key-7 key-8 key-9 key-10
```

```
key-5                CAS=0x158dbe3cb8ce0000, Flags=0x0, Size=7, Datatype=0x00
value-5
key-9                CAS=0x158dbe4c2b3f0000, Flags=0x0, Size=7, Datatype=0x00
value-9
key-10               CAS=0x158dbe4f7deb0000, Flags=0x0, Size=8, Datatype=0x00
value-10
key-2                CAS=0x158dbe2f32620000, Flags=0x0, Size=7, Datatype=0x00
value-2
key-7                CAS=0x158dbe43625d0000, Flags=0x0, Size=7, Datatype=0x00
value-7
key-8                CAS=0x158dbe46c9f60000, Flags=0x0, Size=7, Datatype=0x00
value-8
key-1                CAS=0x158dbe1058080000, Flags=0x0, Size=7, Datatype=0x00
value-1
key-3                CAS=0x158dbe323e320000, Flags=0x0, Size=7, Datatype=0x00
value-3
key-4                LCB_ECONNREFUSED (0x2C)
key-6                LCB_ECONNREFUSED (0x2C)
```

\
내 클러스터에서는 key-4도 Node #3에 위치해 있었기 때문에 읽기가 실패할 것입니다.

```
[ec2-user@AppServer ~]$ cbc-cat -U $NODE3/default key-4
```

```
Failed to bootstrap instance. libcouchbase error: LCB_ECONNREFUSED (0x2C): The remote host refused the connection. Is the service up?
```

하지만 제 환경에서는 key-1과 key-2는 Node #4에 저장되어 있기 때문에 정상적으로 읽을 수 있습니다.

여러분의 환경에서도 유사한 테스트를 실행하여 Node #4의 활성 키를 여전히 읽을 수 있는지 확인해 보세요.

```bash
[ec2-user@AppServer ~]$ cbc-cat -U $NODE4/default  key-1 key-2 key-3 key-4 key-5 key-6 key-7 key-8 key-9 key-10
```

```
key-2                CAS=0x158dbe2f32620000, Flags=0x0, Size=7, Datatype=0x00
value-2
key-7                CAS=0x158dbe43625d0000, Flags=0x0, Size=7, Datatype=0x00
value-7
key-8                CAS=0x158dbe46c9f60000, Flags=0x0, Size=7, Datatype=0x00
value-8
key-1                CAS=0x158dbe1058080000, Flags=0x0, Size=7, Datatype=0x00
value-1
key-3                CAS=0x158dbe323e320000, Flags=0x0, Size=7, Datatype=0x00
value-3
key-5                CAS=0x158dbe3cb8ce0000, Flags=0x0, Size=7, Datatype=0x00
value-5
key-9                CAS=0x158dbe4c2b3f0000, Flags=0x0, Size=7, Datatype=0x00
value-9
key-10               CAS=0x158dbe4f7deb0000, Flags=0x0, Size=8, Datatype=0x00
value-10
key-4                LCB_ECONNREFUSED (0x2C)
key-6                LCB_ECONNREFUSED (0x2C)
```

\
